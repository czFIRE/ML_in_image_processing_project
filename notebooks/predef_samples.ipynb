{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed1a048-1db4-48fd-bc3a-4cd2597f5c9d",
   "metadata": {},
   "source": [
    "# PA228 Project - machine learning in image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12e092-3893-4c29-968b-919214d0ea15",
   "metadata": {},
   "source": [
    "Author: Petr Kadlec, UÄŒO: 485208"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f136e",
   "metadata": {},
   "source": [
    "## Loading the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4d902",
   "metadata": {},
   "source": [
    "Some help 'cause I need it: https://github.com/krasserm/super-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c311f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b477fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7d6eb-52ef-4046-ad15-dd848358354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'Detected gpus: {gpus}')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef643687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ishow(img,\n",
    "          cmap='viridis',\n",
    "          title='',\n",
    "          fig_size=(8,6),\n",
    "          colorbar=True,\n",
    "          interpolation='none'):\n",
    "    ' Function `ishow` displays an image in a new window. '\n",
    "    \n",
    "    extent = (0, img.shape[1], img.shape[0], 0)\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    pcm = ax.imshow(img,\n",
    "              extent=extent,\n",
    "              cmap=cmap,\n",
    "              interpolation=interpolation)\n",
    "    \n",
    "    ax.set_frame_on(False)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if colorbar:\n",
    "        \n",
    "        fig.colorbar(pcm, orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location: str = \"./../dataset/\"\n",
    "\n",
    "training_prefix = dataset_location + \"DIV2K_train_\"\n",
    "validation_prefix = dataset_location + \"DIV2K_valid_\"\n",
    "\n",
    "original_train = training_prefix + \"HR\"\n",
    "original_test = validation_prefix + \"HR\"\n",
    "\n",
    "set_difficult: list[str] = [training_prefix + \"LR_difficult\", original_train, validation_prefix + \"LR_difficult\", original_test]\n",
    "set_mild: list[str] = [training_prefix + \"LR_mild\", original_train, validation_prefix + \"LR_mild\", original_test]\n",
    "set_wild: list[str] = [training_prefix + \"LR_wild\", original_train, validation_prefix + \"LR_wild\", original_test]\n",
    "set_x8: list[str] = [training_prefix + \"LR_x8\", original_train, validation_prefix + \"LR_x8\", original_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 512\n",
    "upscale_factor = 4\n",
    "input_size = crop_size // upscale_factor\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_set: list[str] = set_mild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig_ds = tf.keras.utils.image_dataset_from_directory(current_set[3],\n",
    "                                                 labels=None,\n",
    "                                                 label_mode=\"categorical\",\n",
    "                                                 image_size=(crop_size, crop_size),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 interpolation=\"nearest\",\n",
    "                                                 seed=1,\n",
    "                                                 )\n",
    "\n",
    "test_mod_ds = tf.keras.utils.image_dataset_from_directory(current_set[2],\n",
    "                                                 labels=None,\n",
    "                                                 label_mode=\"categorical\",\n",
    "                                                 image_size=(crop_size, crop_size),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 interpolation=\"nearest\",\n",
    "                                                 seed=1,\n",
    "                                                 )\n",
    "\n",
    "train_orig_ds = tf.keras.utils.image_dataset_from_directory(current_set[1],\n",
    "                                                 labels=None,\n",
    "                                                 label_mode=\"categorical\",\n",
    "                                                 image_size=(crop_size, crop_size),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 interpolation=\"nearest\",\n",
    "                                                 seed=1,\n",
    "                                                 )\n",
    "\n",
    "train_mod_ds = tf.keras.utils.image_dataset_from_directory(current_set[0],\n",
    "                                                 labels=None,\n",
    "                                                 label_mode=\"categorical\",\n",
    "                                                 image_size=(crop_size, crop_size),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 interpolation=\"nearest\",\n",
    "                                                 seed=1,\n",
    "                                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db70a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_images_in_dir(dir_name: str) -> list[str]:\n",
    "    return sorted(\n",
    "        [\n",
    "            os.path.join(dir_name + \"/\", fname)\n",
    "            for fname in os.listdir(dir_name + \"/\")\n",
    "            if fname.endswith(\".png\")\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_mod_ds)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cc05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ishow(list(test_mod_ds)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c118e",
   "metadata": {},
   "source": [
    "Rescale all the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(input_image):\n",
    "    input_image = input_image / 255.0\n",
    "    return input_image\n",
    "\n",
    "#tmp = ds_test_img.map(lambda x: tf.cast(x, tf.float32))\n",
    "#ds_test_img = tmp.map(scaling)\n",
    "\n",
    "test_mod_ds = test_mod_ds.map(lambda x: tf.cast(x, tf.float32)).map(scaling)\n",
    "test_orig_ds = test_orig_ds.map(lambda x: tf.cast(x, tf.float32)).map(scaling)\n",
    "train_mod_ds = train_mod_ds.map(lambda x: tf.cast(x, tf.float32)).map(scaling)\n",
    "train_orig_ds = train_orig_ds.map(lambda x: tf.cast(x, tf.float32)).map(scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_orig_ds)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ishow(list(test_orig_ds)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a792d",
   "metadata": {},
   "source": [
    "## Crop and resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF Ops to process.\n",
    "def process_input(input, input_size, upscale_factor):\n",
    "    input = tf.image.rgb_to_yuv(input)\n",
    "    last_dimension_axis = len(input.shape) - 1\n",
    "    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n",
    "    return tf.image.resize(y, [input_size, input_size], method=\"area\")\n",
    "\n",
    "\n",
    "def process_target(input):\n",
    "    input = tf.image.rgb_to_yuv(input)\n",
    "    last_dimension_axis = len(input.shape) - 1\n",
    "    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265175ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here be dragons => here is the place to rewrite it\n",
    "\n",
    "train_mod_ds = train_mod_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "train_mod_ds = train_mod_ds.prefetch(buffer_size=32)\n",
    "\n",
    "train_orig_ds = train_orig_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "train_orig_ds = train_orig_ds.prefetch(buffer_size=32)\n",
    "\n",
    "test_mod_ds = test_mod_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "test_mod_ds = test_mod_ds.prefetch(buffer_size=32)\n",
    "\n",
    "test_orig_ds = test_orig_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "test_orig_ds = test_orig_ds.prefetch(buffer_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee793b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from IPython.display import display\n",
    "\n",
    "for batch in train_mod_ds.take(1):\n",
    "    for img in batch[0]:\n",
    "        display(array_to_img(img))\n",
    "    for img in batch[1]:\n",
    "        display(array_to_img(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab10202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(upscale_factor=3, channels=1):\n",
    "    conv_args = {\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_initializer\": \"Orthogonal\",\n",
    "        \"padding\": \"same\",\n",
    "    }\n",
    "    inputs = tf.keras.Input(shape=(None, None, channels))\n",
    "    x = tf.keras.layers.Conv2D(64, 5, **conv_args)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, **conv_args)(x)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, **conv_args)(x)\n",
    "    x = tf.keras.layers.Conv2D(channels * (upscale_factor ** 2), 3, **conv_args)(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscale_factor)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587847ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import PIL\n",
    "\n",
    "\n",
    "def plot_results(img, prefix, title):\n",
    "    \"\"\"Plot the result with zoom-in area.\"\"\"\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Create a new figure with a default 111 subplot.\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(img_array[::-1], origin=\"lower\")\n",
    "\n",
    "    plt.title(title)\n",
    "    # zoom-factor: 2.0, location: upper-left\n",
    "    axins = zoomed_inset_axes(ax, 2, loc=2)\n",
    "    axins.imshow(img_array[::-1], origin=\"lower\")\n",
    "\n",
    "    # Specify the limits.\n",
    "    x1, x2, y1, y2 = 200, 300, 100, 200\n",
    "    # Apply the x-limits.\n",
    "    axins.set_xlim(x1, x2)\n",
    "    # Apply the y-limits.\n",
    "    axins.set_ylim(y1, y2)\n",
    "\n",
    "    plt.yticks(visible=False)\n",
    "    plt.xticks(visible=False)\n",
    "\n",
    "    # Make the line.\n",
    "    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")\n",
    "    # plt.savefig(str(prefix) + \"-\" + title + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_lowres_image(img, upscale_factor):\n",
    "    \"\"\"Return low-resolution image to use as model input.\"\"\"\n",
    "    return img.resize(\n",
    "        (img.size[0] // upscale_factor, img.size[1] // upscale_factor),\n",
    "        PIL.Image.Resampling.BICUBIC,\n",
    "    )\n",
    "\n",
    "\n",
    "def upscale_image(model, img):\n",
    "    \"\"\"Predict the result based on input image and restore the image as RGB.\"\"\"\n",
    "    ycbcr = img.convert(\"YCbCr\")\n",
    "    y, cb, cr = ycbcr.split()\n",
    "    y = img_to_array(y)\n",
    "    y = y.astype(\"float32\") / 255.0\n",
    "\n",
    "    input = np.expand_dims(y, axis=0)\n",
    "    out = model.predict(input)\n",
    "\n",
    "    out_img_y = out[0]\n",
    "    out_img_y *= 255.0\n",
    "\n",
    "    # Restore the image in RGB color space.\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = out_img_y.reshape((np.shape(out_img_y)[0], np.shape(out_img_y)[1]))\n",
    "    out_img_y = PIL.Image.fromarray(np.uint8(out_img_y), mode=\"L\")\n",
    "    out_img_cb = cb.resize(out_img_y.size, PIL.Image.Resampling.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, PIL.Image.Resampling.BICUBIC)\n",
    "    out_img = PIL.Image.merge(\"YCbCr\", (out_img_y, out_img_cb, out_img_cr)).convert(\n",
    "        \"RGB\"\n",
    "    )\n",
    "    return out_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_test_paths = get_images_in_dir(current_set[2])\n",
    "modified_test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import math\n",
    "\n",
    "class ESPCNCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(ESPCNCallback, self).__init__()\n",
    "        #self.test_img = get_lowres_image(load_img(original_test + \"/0801.png\"), upscale_factor)\n",
    "        self.test_img = load_img(modified_test_paths[0])\n",
    "\n",
    "    # Store PSNR value in each epoch.\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.psnr = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Mean PSNR for epoch: %.2f\" % (np.mean(self.psnr)))\n",
    "        if epoch % 20 == 0:\n",
    "            prediction = upscale_image(self.model, self.test_img)\n",
    "            plot_results(prediction, \"epoch-\" + str(epoch), \"prediction\")\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        self.psnr.append(10 * math.log10(1 / logs[\"loss\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n",
    "\n",
    "checkpoint_filepath = \"./tmp/checkpoint\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "model = get_model(upscale_factor=upscale_factor, channels=1)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback]\n",
    "#callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=loss_fn,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_orig_ds, epochs=epochs, callbacks=callbacks, validation_data=test_orig_ds, verbose=2\n",
    ")\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "model.load_weights(checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_paths = get_images_in_dir(original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c607601",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bicubic_psnr = 0.0\n",
    "total_test_psnr = 0.0\n",
    "\n",
    "for index, test_img_path in enumerate(test_img_paths[10:20]):\n",
    "    img = load_img(test_img_path)\n",
    "    lowres_input = get_lowres_image(img, upscale_factor)\n",
    "    w = lowres_input.size[0] * upscale_factor\n",
    "    h = lowres_input.size[1] * upscale_factor\n",
    "    highres_img = img.resize((w, h))\n",
    "    prediction = upscale_image(model, lowres_input)\n",
    "    lowres_img = lowres_input.resize((w, h))\n",
    "    lowres_img_arr = img_to_array(lowres_img)\n",
    "    highres_img_arr = img_to_array(highres_img)\n",
    "    predict_img_arr = img_to_array(prediction)\n",
    "    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n",
    "    test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)\n",
    "\n",
    "    total_bicubic_psnr += bicubic_psnr\n",
    "    total_test_psnr += test_psnr\n",
    "\n",
    "    print(\n",
    "        \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n",
    "    )\n",
    "    print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)\n",
    "    plot_results(lowres_img, index, \"lowres\")\n",
    "    plot_results(highres_img, index, \"highres\")\n",
    "    plot_results(prediction, index, \"prediction\")\n",
    "\n",
    "print(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr / 10))\n",
    "print(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr / 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322c9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
