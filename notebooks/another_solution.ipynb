{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some help 'cause I need it: https://github.com/krasserm/super-resolution\n",
    "\n",
    "https://github.com/idealo/image-super-resolution/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def resolve_single(model, lr):\n",
    "    return resolve(model, tf.expand_dims(lr, axis=0))[0]\n",
    "\n",
    "def resolve(model, lr_batch):\n",
    "    lr_batch = tf.cast(lr_batch, tf.float32)\n",
    "    sr_batch = model(lr_batch)\n",
    "    sr_batch = tf.clip_by_value(sr_batch, 0, 255)\n",
    "    sr_batch = tf.round(sr_batch)\n",
    "    sr_batch = tf.cast(sr_batch, tf.uint8)\n",
    "    return sr_batch\n",
    "\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    psnr_values = []\n",
    "    for lr, hr in dataset:\n",
    "        sr = resolve(model, lr)\n",
    "        psnr_value = psnr(hr, sr)[0]\n",
    "        psnr_values.append(psnr_value)\n",
    "    return tf.reduce_mean(psnr_values)\n",
    "\n",
    "def psnr(x1, x2):\n",
    "    return tf.image.psnr(x1, x2, max_val=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Add, Conv2D, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\n",
    "\n",
    "\n",
    "def edsr(scale, num_filters=64, num_res_blocks=8, res_block_scaling=None):\n",
    "    \"\"\"Creates an EDSR model.\"\"\"\n",
    "    x_in = Input(shape=(None, None, 3))\n",
    "    x = Lambda(normalize)(x_in)\n",
    "\n",
    "    x = b = Conv2D(num_filters, 3, padding='same')(x)\n",
    "    for i in range(num_res_blocks):\n",
    "        b = res_block(b, num_filters, res_block_scaling)\n",
    "    b = Conv2D(num_filters, 3, padding='same')(b)\n",
    "    x = Add()([x, b])\n",
    "\n",
    "    x = upsample(x, scale, num_filters)\n",
    "    x = Conv2D(3, 3, padding='same')(x)\n",
    "\n",
    "    x = Lambda(denormalize)(x)\n",
    "    return Model(x_in, x, name=\"edsr\")\n",
    "\n",
    "\n",
    "def res_block(x_in, filters, scaling):\n",
    "    \"\"\"Creates an EDSR residual block.\"\"\"\n",
    "    x = Conv2D(filters, 3, padding='same', activation='relu')(x_in)\n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    if scaling:\n",
    "        x = Lambda(lambda t: t * scaling)(x)\n",
    "    x = Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(x, scale, num_filters):\n",
    "    def upsample_1(x, factor, **kwargs):\n",
    "        \"\"\"Sub-pixel convolution.\"\"\"\n",
    "        x = Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
    "        return Lambda(pixel_shuffle(scale=factor))(x)\n",
    "\n",
    "    if scale == 2:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "    elif scale == 3:\n",
    "        x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
    "    elif scale == 4:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "        x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def pixel_shuffle(scale):\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - DIV2K_RGB_MEAN) / 127.5\n",
    "\n",
    "\n",
    "def denormalize(x):\n",
    "    return x * 127.5 + DIV2K_RGB_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 loss,\n",
    "                 learning_rate,\n",
    "                 checkpoint_dir='./ckpt/edsr'):\n",
    "\n",
    "        self.now = None\n",
    "        self.loss = loss\n",
    "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(0),\n",
    "                                              psnr=tf.Variable(-1.0),\n",
    "                                              optimizer=Adam(learning_rate),\n",
    "                                              model=model)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(checkpoint=self.checkpoint,\n",
    "                                                             directory=checkpoint_dir,\n",
    "                                                             max_to_keep=3)\n",
    "\n",
    "        self.restore()\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.checkpoint.model\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset, steps, evaluate_every=1000, save_best_only=False):\n",
    "        loss_mean = Mean()\n",
    "\n",
    "        ckpt_mgr = self.checkpoint_manager\n",
    "        ckpt = self.checkpoint\n",
    "\n",
    "        self.now = time.perf_counter()\n",
    "\n",
    "        for lr, hr in train_dataset.take(steps - ckpt.step.numpy()):\n",
    "            ckpt.step.assign_add(1)\n",
    "            step = ckpt.step.numpy()\n",
    "\n",
    "            loss = self.train_step(lr, hr)\n",
    "            loss_mean(loss)\n",
    "\n",
    "            if step % evaluate_every == 0:\n",
    "                loss_value = loss_mean.result()\n",
    "                loss_mean.reset_states()\n",
    "\n",
    "                # Compute PSNR on validation dataset\n",
    "                psnr_value = self.evaluate(valid_dataset)\n",
    "\n",
    "                duration = time.perf_counter() - self.now\n",
    "                print(f'{step}/{steps}: loss = {loss_value.numpy():.3f}, PSNR = {psnr_value.numpy():3f} ({duration:.2f}s)')\n",
    "\n",
    "                if save_best_only and psnr_value <= ckpt.psnr:\n",
    "                    self.now = time.perf_counter()\n",
    "                    # skip saving checkpoint, no PSNR improvement\n",
    "                    continue\n",
    "\n",
    "                ckpt.psnr = psnr_value\n",
    "                ckpt_mgr.save()\n",
    "\n",
    "                self.now = time.perf_counter()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, lr, hr):\n",
    "        with tf.GradientTape() as tape:\n",
    "            lr = tf.cast(lr, tf.float32)\n",
    "            hr = tf.cast(hr, tf.float32)\n",
    "\n",
    "            sr = self.checkpoint.model(lr, training=True)\n",
    "            loss_value = self.loss(hr, sr)\n",
    "\n",
    "        gradients = tape.gradient(loss_value, self.checkpoint.model.trainable_variables)\n",
    "        self.checkpoint.optimizer.apply_gradients(zip(gradients, self.checkpoint.model.trainable_variables))\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        return evaluate(self.checkpoint.model, dataset)\n",
    "\n",
    "    def restore(self):\n",
    "        if self.checkpoint_manager.latest_checkpoint:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            print(f'Model restored from checkpoint at step {self.checkpoint.step.numpy()}.')\n",
    "\n",
    "\n",
    "class EdsrTrainer(Trainer):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 checkpoint_dir,\n",
    "                 learning_rate=PiecewiseConstantDecay(boundaries=[200000], values=[1e-4, 5e-5])):\n",
    "        super().__init__(model, loss=MeanAbsoluteError(), learning_rate=learning_rate, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset, steps=300000, evaluate_every=1000, save_best_only=True):\n",
    "        super().train(train_dataset, valid_dataset, steps, evaluate_every, save_best_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of residual blocks\n",
    "depth = 16\n",
    "\n",
    "# Super-resolution factor\n",
    "scale = 4\n",
    "\n",
    "# Downgrade operator\n",
    "downgrade = 'mild'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of model weights\n",
    "weights_dir = f'weights/edsr-{depth}-x{scale}'\n",
    "weights_file = os.path.join(weights_dir, 'weights.h5')\n",
    "\n",
    "os.makedirs(weights_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DIV2K\n",
    "\n",
    "div2k_train = DIV2K(scale=scale, subset='train', downgrade=downgrade, images_dir=\"./../dataset/\", caches_dir=\"./../dataset/\")\n",
    "div2k_valid = DIV2K(scale=scale, subset='valid', downgrade=downgrade, images_dir=\"./../dataset/\", caches_dir=\"./../dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = div2k_train.dataset(batch_size=16, random_transform=True)\n",
    "valid_ds = div2k_valid.dataset(batch_size=1, random_transform=False, repeat_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = EdsrTrainer(model=edsr(scale=scale, num_res_blocks=depth), \n",
    "                      checkpoint_dir=f'.ckpt/edsr-{depth}-x{scale}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(trainer.model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_ds,\n",
    "              valid_ds.take(10),\n",
    "              steps=10000, \n",
    "              evaluate_every=50, \n",
    "              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore from checkpoint with highest PSNR\n",
    "trainer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on full validation set\n",
    "psnrv = trainer.evaluate(valid_ds)\n",
    "print(f'PSNR = {psnrv.numpy():3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to separate location (needed for demo)\n",
    "trainer.model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = edsr(scale=scale, num_res_blocks=depth)\n",
    "model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "\n",
    "def plot_sample(lr, sr):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    images = [lr, sr]\n",
    "    titles = ['LR', f'SR (x{sr.shape[0] // lr.shape[0]})']\n",
    "\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_and_plot(lr_image_path):\n",
    "    lr = load_image(lr_image_path)\n",
    "    sr = resolve_single(model, lr)\n",
    "    plot_sample(lr, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_and_plot('./../dataset/DIV2K_valid_LR_mild/0869x4m.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_and_plot('./../dataset/DIV2K_valid_LR_mild/0829x4m.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_and_plot('./../dataset/DIV2K_valid_LR_mild/0851x4m.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
