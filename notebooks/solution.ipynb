{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed1a048-1db4-48fd-bc3a-4cd2597f5c9d",
   "metadata": {},
   "source": [
    "# PA228 Project - machine learning in image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12e092-3893-4c29-968b-919214d0ea15",
   "metadata": {},
   "source": [
    "Author: Petr Kadlec, UÄŒO: 485208"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f136e",
   "metadata": {},
   "source": [
    "## Loading the dataset:\n",
    "\n",
    "The basic structure of this code was taken from https://keras.io/examples/vision/super_resolution_sub_pixel/.\n",
    "\n",
    "The definition of EDSR model was done using the ideas from the paper found here: https://arxiv.org/pdf/1707.02921.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a6e9f",
   "metadata": {},
   "source": [
    "In this project we are using three models:\n",
    "\n",
    "- baseline:           As baseline we are using _rescale_ method from skimage\n",
    "- basic model:        This model is only composed of a few CNN layers, this is just so we have a comparison if we truly need a bigger model.\n",
    "- EDSR model:         This model is the main \"star of the show\". It's architecture is explained later in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4d902",
   "metadata": {},
   "source": [
    "Basic functions and loading of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c311f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable MYPY in the jupyter notebook => commented out because then I wouldn't be able to do y = y.astype(\"float32\") and similar\n",
    "#%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b477fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7d6eb-52ef-4046-ad15-dd848358354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'Detected gpus: {gpus}')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c3db2",
   "metadata": {},
   "source": [
    "Basic downloader utility for all the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8645706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import wget\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "basic_url: str = \"https://data.vision.ee.ethz.ch/cvl/DIV2K/\"\n",
    "\n",
    "print(\"\\nDownloading and extracting dataset to this directory.\\n\")\n",
    "\n",
    "datasets = [\"DIV2K_train_HR\", \"DIV2K_valid_HR\"]\n",
    "\n",
    "for line in datasets:\n",
    "    # if you don't have wget module then uncomment the next line and comment the wget command\n",
    "    # os.system(\"wget \" + basic_url + line[:-1] + \".zip\")\n",
    "    wget.download(basic_url + line[:-1] + \".zip\")\n",
    "\n",
    "    shutil.unpack_archive(\"./\" + line[:-1] + \".zip\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location: str = \"./../dataset/\"\n",
    "\n",
    "training_prefix = dataset_location + \"DIV2K_train_\"\n",
    "validation_prefix = dataset_location + \"DIV2K_valid_\"\n",
    "\n",
    "original_train = training_prefix + \"HR\"\n",
    "original_test = validation_prefix + \"HR\"\n",
    "\n",
    "set_difficult: list[str] = [training_prefix + \"LR_difficult\", original_train, validation_prefix + \"LR_difficult\", original_test, \"4\"]\n",
    "set_mild: list[str] = [training_prefix + \"LR_mild\", original_train, validation_prefix + \"LR_mild\", original_test, \"4\"]\n",
    "set_wild: list[str] = [training_prefix + \"LR_wild\", original_train, validation_prefix + \"LR_wild\", original_test, \"4\"]\n",
    "set_x8: list[str] = [training_prefix + \"LR_x8\", original_train, validation_prefix + \"LR_x8\", original_test, \"8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_set: list[str] = set_mild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig_ds = tf.keras.utils.image_dataset_from_directory(current_set[3],\n",
    "                                                 labels=None,\n",
    "                                                 label_mode=\"categorical\",\n",
    "                                                 image_size=(crop_size, crop_size),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 interpolation=\"nearest\",\n",
    "                                                 seed=1,\n",
    "                                                 )\n",
    "\n",
    "train_orig_ds = tf.keras.utils.image_dataset_from_directory(current_set[1],\n",
    "                                                 labels=None,\n",
    "                                                 label_mode=\"categorical\",\n",
    "                                                 image_size=(crop_size, crop_size),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 interpolation=\"nearest\",\n",
    "                                                 validation_split=0.2,\n",
    "                                                 subset=\"training\",\n",
    "                                                 seed=1,\n",
    "                                                 )\n",
    "\n",
    "validation_orig_ds = tf.keras.utils.image_dataset_from_directory(current_set[1],\n",
    "                                                 labels=None,\n",
    "                                                 label_mode=\"categorical\",\n",
    "                                                 image_size=(crop_size, crop_size),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 interpolation=\"nearest\",\n",
    "                                                 validation_split=0.2,\n",
    "                                                 subset=\"validation\",\n",
    "                                                 seed=1,\n",
    "                                                 )\n",
    "\n",
    "upscale_factor = int(current_set[4])\n",
    "input_size = crop_size // upscale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db70a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_images_in_dir(dir_name: str) -> list[str]:\n",
    "    return sorted(\n",
    "        [\n",
    "            os.path.join(dir_name + \"/\", fname)\n",
    "            for fname in os.listdir(dir_name + \"/\")\n",
    "            if fname.endswith(\".png\")\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c118e",
   "metadata": {},
   "source": [
    "Rescale all the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(input_image):\n",
    "    input_image = input_image / 255.0\n",
    "    return input_image\n",
    "\n",
    "test_orig_ds = test_orig_ds.map(lambda x: tf.cast(x, tf.float32)).map(scaling)\n",
    "validation_orig_ds = validation_orig_ds.map(lambda x: tf.cast(x, tf.float32)).map(scaling)\n",
    "train_orig_ds = train_orig_ds.map(lambda x: tf.cast(x, tf.float32)).map(scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a792d",
   "metadata": {},
   "source": [
    "## Crop and resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF Ops to process.\n",
    "\n",
    "# Only the y channel is interesting as people are the most sensitive to it\n",
    "def process_input(input, input_size, upscale_factor):\n",
    "    input = tf.image.rgb_to_yuv(input)\n",
    "    last_dimension_axis = len(input.shape) - 1\n",
    "    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n",
    "    return tf.image.resize(y, [input_size, input_size], method=\"area\")\n",
    "\n",
    "\n",
    "def process_target(input):\n",
    "    input = tf.image.rgb_to_yuv(input)\n",
    "    last_dimension_axis = len(input.shape) - 1\n",
    "    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265175ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig_ds = train_orig_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "train_orig_ds = train_orig_ds.prefetch(buffer_size=32)\n",
    "\n",
    "test_orig_ds = test_orig_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "test_orig_ds = test_orig_ds.prefetch(buffer_size=32)\n",
    "\n",
    "\n",
    "validation_orig_ds = validation_orig_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "validation_orig_ds = validation_orig_ds.prefetch(buffer_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6a66a",
   "metadata": {},
   "source": [
    "Defining the basic model used for upscaling (this is basically a basic model, not baseline!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab10202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(upscale_factor=3, channels=1):\n",
    "    conv_args = {\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_initializer\": \"Orthogonal\",\n",
    "        \"padding\": \"same\",\n",
    "    }\n",
    "    inputs = tf.keras.Input(shape=(None, None, channels))\n",
    "    x = tf.keras.layers.Conv2D(64, 5, **conv_args)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, **conv_args)(x)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, **conv_args)(x)\n",
    "    x = tf.keras.layers.Conv2D(channels * (upscale_factor ** 2), 3, **conv_args)(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscale_factor)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587847ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import PIL\n",
    "\n",
    "\n",
    "def plot_results(img, prefix, title, directory = None):\n",
    "    \"\"\"Plot the result with zoom-in area.\"\"\"\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Create a new figure with a default 111 subplot.\n",
    "    fig, ax = plt.subplots(figsize=(20,20))\n",
    "    im = ax.imshow(img_array[::-1], origin=\"lower\")\n",
    "\n",
    "    plt.title(title)\n",
    "    # zoom-factor: 2.0, location: upper-left\n",
    "    axins = zoomed_inset_axes(ax, 2, loc=2)\n",
    "    axins.imshow(img_array[::-1], origin=\"lower\")\n",
    "\n",
    "    # Specify the limits.\n",
    "    x1, x2, y1, y2 = 200, 300, 100, 200\n",
    "    # Apply the x-limits.\n",
    "    axins.set_xlim(x1, x2)\n",
    "    # Apply the y-limits.\n",
    "    axins.set_ylim(y1, y2)\n",
    "\n",
    "    plt.yticks(visible=False)\n",
    "    plt.xticks(visible=False)\n",
    "\n",
    "    # Make the line.\n",
    "    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")\n",
    "    if (directory is not None): \n",
    "        plt.savefig(\"./output_images/\" + directory + str(prefix) + \"-\" + title + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_lowres_image(img, upscale_factor):\n",
    "    \"\"\"Return low-resolution image to use as model input.\"\"\"\n",
    "    return img.resize(\n",
    "        (img.size[0] // upscale_factor, img.size[1] // upscale_factor),\n",
    "        PIL.Image.Resampling.BICUBIC,\n",
    "    )\n",
    "\n",
    "\n",
    "def upscale_image(model, img):\n",
    "    \"\"\"Predict the result based on input image and restore the image as RGB.\"\"\"\n",
    "    ycbcr = img.convert(\"YCbCr\")\n",
    "    y, cb, cr = ycbcr.split()\n",
    "    y = img_to_array(y)\n",
    "    y = y.astype(\"float32\") / 255.0\n",
    "\n",
    "    input = np.expand_dims(y, axis=0)\n",
    "    out = model.predict(input)\n",
    "\n",
    "    out_img_y = out[0]\n",
    "    out_img_y *= 255.0\n",
    "\n",
    "    # Restore the image in RGB color space.\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = out_img_y.reshape((np.shape(out_img_y)[0], np.shape(out_img_y)[1]))\n",
    "    out_img_y = PIL.Image.fromarray(np.uint8(out_img_y), mode=\"L\")\n",
    "    out_img_cb = cb.resize(out_img_y.size, PIL.Image.Resampling.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, PIL.Image.Resampling.BICUBIC)\n",
    "    out_img = PIL.Image.merge(\"YCbCr\", (out_img_y, out_img_cb, out_img_cr)).convert(\n",
    "        \"RGB\"\n",
    "    )\n",
    "    return out_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff14907",
   "metadata": {},
   "source": [
    "Defining the main matric for evaluating the images => PSNR\n",
    "\n",
    "PSNR stands for peak signal-to-noise ratio and in general the higher the PSNR, the better quality of the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import math\n",
    "\n",
    "class ESPCNCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(ESPCNCallback, self).__init__()\n",
    "        self.test_img = get_lowres_image(load_img(original_test + \"/0801.png\"), upscale_factor)\n",
    "\n",
    "    # Store PSNR value in each epoch.\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.psnr = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Mean PSNR for epoch: %.2f\" % (np.mean(self.psnr)))\n",
    "        if epoch % 20 == 0:\n",
    "            prediction = upscale_image(self.model, self.test_img)\n",
    "            plot_results(prediction, \"epoch-\" + str(epoch), \"prediction\", \"training/\")\n",
    "\n",
    "    # Computing PSNR from definition here\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        self.psnr.append(10 * math.log10(1 / logs[\"loss\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_paths = get_images_in_dir(original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3067a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "log_dir = 'logs'\n",
    "shutil.rmtree(log_dir)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir + '/basic', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "checkpoint_filepath = \"./tmp/checkpoint/basic\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "model_basic = get_model(upscale_factor=upscale_factor, channels=1)\n",
    "model_basic.summary()\n",
    "\n",
    "callbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback, tensorboard_callback]\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8918b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_basic, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(y_true, y_pred):\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "model_basic.compile(\n",
    "    optimizer=optimizer, loss=loss_fn, metrics=[PSNR]\n",
    ")\n",
    "\n",
    "\n",
    "# split the dataset to validation and training\n",
    "train_log = model_basic.fit(\n",
    "    train_orig_ds, epochs=epochs, callbacks=callbacks, validation_data=validation_orig_ds, verbose=2\n",
    ")\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "model_basic.load_weights(checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "names = train_log.history.keys()\n",
    "\n",
    "for name in names:\n",
    "    plt.plot(train_log.history[name])\n",
    "\n",
    "plt.title(\"training history\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99505b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/basic --host localhost --port 8081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c607601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rescale\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "total_bicubic_psnr = 0.0\n",
    "total_test_psnr = 0.0\n",
    "total_baseline_psnr = 0.0\n",
    "\n",
    "model_prefix: str = \"model_easy/\"\n",
    "model = model_basic\n",
    "\n",
    "for index, test_img_path in enumerate(test_img_paths[30:40]):\n",
    "    img = load_img(test_img_path)\n",
    "    lowres_input = get_lowres_image(img, upscale_factor)\n",
    "    w = lowres_input.size[0] * upscale_factor\n",
    "    h = lowres_input.size[1] * upscale_factor\n",
    "    highres_img = img.resize((w, h))\n",
    "    prediction = upscale_image(model, lowres_input)\n",
    "    lowres_img = lowres_input.resize((w, h))\n",
    "    lowres_img_arr = img_to_array(lowres_img)\n",
    "    highres_img_arr = img_to_array(highres_img)\n",
    "    predict_img_arr = img_to_array(prediction)\n",
    "\n",
    "    baseline_rescaled = array_to_img(rescale(img_to_array(lowres_input), (upscale_factor, upscale_factor, 1)))\n",
    "    \n",
    "    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n",
    "    test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)\n",
    "    baseline_psnr = tf.image.psnr(img_to_array(baseline_rescaled), highres_img_arr, max_val=255)\n",
    "\n",
    "    total_bicubic_psnr += bicubic_psnr\n",
    "    total_test_psnr += test_psnr\n",
    "    total_baseline_psnr += baseline_psnr\n",
    "\n",
    "    print(\n",
    "        \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n",
    "    )\n",
    "    print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)\n",
    "    print(\"PSNR of baseline and high resolution is %.4f\" % baseline_psnr)\n",
    "    plot_results(lowres_img, index, \"lowres\", model_prefix)\n",
    "    plot_results(highres_img, index, \"highres\", model_prefix)\n",
    "    plot_results(prediction, index, \"prediction\", model_prefix)\n",
    "    plot_results(baseline_rescaled, index, \"baseline\", model_prefix)\n",
    "\n",
    "print(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr / 10))\n",
    "print(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr / 10))\n",
    "print(\"Avg. PSNR of baseline is %.4f\" % (total_baseline_psnr / 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140c1e4",
   "metadata": {},
   "source": [
    "As you can see, our basic model is only slightly better than the baseline.\n",
    "\n",
    "However, I would still argue, that the uspcaling of the image was successfull, however the model has mostly learned to \"smear\" the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692e24d",
   "metadata": {},
   "source": [
    "## EDSR model:\n",
    "\n",
    "[Enhanced Deep Residual Networks for Single Image Super-Resolution](https://arxiv.org/abs/1707.02921) (EDSR) is a winner of the [NTIRE 2017](http://www.vision.ee.ethz.ch/ntire17/) super-resolution challenge. Here's an overview of the EDSR architecture:\n",
    "\n",
    "![Fig. 1](edsr_archi.png)\n",
    "<center>Fig. 1. EDSR architecture.</center>\n",
    "\n",
    "Its residual block design differs from that of ResNet. Batch normalization layers have been removed together with the final ReLU activation as shown on the right side of the next figure (Fig. 2).  \n",
    "\n",
    "![Fig. 2](blocks_diff.png)\n",
    "<center>Fig. 2. Residual block design in ResNet (left) and in EDSR (right).</center>\n",
    "\n",
    "The EDSR authors argue that batch normalization loses scale information of images and reduces the range flexibility of activations. Removal of batch normalization layers not only increases super-resolution performance but also reduces GPU memory up to 40%, thus significantly larger models can be trained.\n",
    "\n",
    "EDSR uses a single sub-pixel upsampling layer for super-resolution scales (i.e. upsampling factors) $\\times 2$ and $\\times 3$ and two upsampling layers for scale $\\times 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e368348",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_args = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"kernel_initializer\": \"Orthogonal\",\n",
    "    \"padding\": \"same\",\n",
    "}\n",
    "\n",
    "def edsr(scale, num_filters=64, num_res_blocks=8, res_block_scaling=None, channels=1):\n",
    "    \"\"\"Creates an EDSR model.\"\"\"\n",
    "    x_in = tf.keras.layers.Input(shape=(None, None, channels))\n",
    "\n",
    "    x = b = tf.keras.layers.Conv2D(num_filters, 3, **conv_args)(x_in)\n",
    "    for i in range(num_res_blocks):\n",
    "        b = res_block(b, num_filters, res_block_scaling)\n",
    "    b = tf.keras.layers.Conv2D(num_filters, 3, **conv_args)(b)\n",
    "    x = tf.keras.layers.Add()([x, b])\n",
    "\n",
    "    x = upsample(x, scale, num_filters)\n",
    "    x = tf.keras.layers.Conv2D(channels, 3, **conv_args)(x)\n",
    "\n",
    "    return tf.keras.models.Model(x_in, x, name=\"edsr\")\n",
    "\n",
    "\n",
    "def res_block(x_in, filters, scaling):\n",
    "    \"\"\"Creates an EDSR residual block.\"\"\"\n",
    "    x = tf.keras.layers.Conv2D(filters, 3, **conv_args)(x_in)\n",
    "    x = tf.keras.layers.Conv2D(filters, 3, **conv_args)(x)\n",
    "    if scaling:\n",
    "        x = tf.keras.layers.Lambda(lambda t: t * scaling)(x)\n",
    "    x = tf.keras.layers.Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(x, scale, num_filters):\n",
    "    def upsample_1(x, factor, **kwargs):\n",
    "        \"\"\"Sub-pixel convolution.\"\"\"\n",
    "        x = tf.keras.layers.Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
    "        return tf.keras.layers.Lambda(pixel_shuffle(scale=factor))(x)\n",
    "\n",
    "    if scale == 2:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "    elif scale == 3:\n",
    "        x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
    "    elif scale == 4:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "        x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def pixel_shuffle(scale):\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cf6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir + '/edsr', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "checkpoint_filepath = \"./tmp/checkpoint/edsr\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "model_edsr = edsr(upscale_factor)\n",
    "model_edsr.summary()\n",
    "\n",
    "callbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback, tensorboard_callback]\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d58ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_edsr, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "model_edsr.compile(\n",
    "    optimizer=optimizer, loss=loss_fn, metrics=[PSNR]\n",
    ")\n",
    "\n",
    "\n",
    "# split the dataset to validation and training\n",
    "train_log = model_edsr.fit(\n",
    "    train_orig_ds, epochs=epochs, callbacks=callbacks, validation_data=validation_orig_ds, verbose=2\n",
    ")\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "model_edsr.load_weights(checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2636003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_edsr.save('pa228_project.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "names = train_log.history.keys()\n",
    "\n",
    "for name in names:\n",
    "    plt.plot(train_log.history[name])\n",
    "\n",
    "plt.title(\"training history\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c79f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/edsr --host localhost --port 8082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c607601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rescale\n",
    "\n",
    "total_bicubic_psnr = 0.0\n",
    "total_test_psnr = 0.0\n",
    "total_baseline_psnr = 0.0\n",
    "\n",
    "model_prefix: str = \"model_edsr/\"\n",
    "model = model_edsr\n",
    "\n",
    "for index, test_img_path in enumerate(test_img_paths[30:40]):\n",
    "    img = load_img(test_img_path)\n",
    "    lowres_input = get_lowres_image(img, upscale_factor)\n",
    "    w = lowres_input.size[0] * upscale_factor\n",
    "    h = lowres_input.size[1] * upscale_factor\n",
    "    highres_img = img.resize((w, h))\n",
    "    prediction = upscale_image(model, lowres_input)\n",
    "    lowres_img = lowres_input.resize((w, h))\n",
    "    lowres_img_arr = img_to_array(lowres_img)\n",
    "    highres_img_arr = img_to_array(highres_img)\n",
    "    predict_img_arr = img_to_array(prediction)\n",
    "\n",
    "    baseline_rescaled = array_to_img(rescale(img_to_array(lowres_input), (upscale_factor, upscale_factor, 1)))\n",
    "    \n",
    "    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n",
    "    test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)\n",
    "    baseline_psnr = tf.image.psnr(img_to_array(baseline_rescaled), highres_img_arr, max_val=255)\n",
    "\n",
    "    total_bicubic_psnr += bicubic_psnr\n",
    "    total_test_psnr += test_psnr\n",
    "    total_baseline_psnr += baseline_psnr\n",
    "\n",
    "    print(\n",
    "        \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n",
    "    )\n",
    "    print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)\n",
    "    print(\"PSNR of baseline and high resolution is %.4f\" % baseline_psnr)\n",
    "    plot_results(lowres_img, index, \"lowres\", model_prefix)\n",
    "    plot_results(highres_img, index, \"highres\", model_prefix)\n",
    "    plot_results(prediction, index, \"prediction\", model_prefix)\n",
    "    plot_results(baseline_rescaled, index, \"baseline\", model_prefix)\n",
    "\n",
    "print(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr / 10))\n",
    "print(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr / 10))\n",
    "print(\"Avg. PSNR of baseline is %.4f\" % (total_baseline_psnr / 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37dba99",
   "metadata": {},
   "source": [
    "I would say the predictions are relatively good, as the whole image looks really sharper and not smeared / distorted. The model mostly learned to smear the images, but to a lesser degree than the simple model and we can sometimes see the sharp edges and clear lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50025f26",
   "metadata": {},
   "source": [
    "## Evaluation over the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basic.evaluate(test_orig_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66574b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_edsr.evaluate(test_orig_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bicubic_psnr = []\n",
    "total_edsr_psnr = []\n",
    "total_baseline_psnr = []\n",
    "total_basic_psnr = []\n",
    "\n",
    "for index, test_img_path in enumerate(test_img_paths):\n",
    "    img = load_img(test_img_path)\n",
    "    lowres_input = get_lowres_image(img, upscale_factor)\n",
    "    w = lowres_input.size[0] * upscale_factor\n",
    "    h = lowres_input.size[1] * upscale_factor\n",
    "    highres_img = img.resize((w, h))\n",
    "    prediction_basic = upscale_image(model_basic, lowres_input)\n",
    "    prediction_edsr = upscale_image(model_edsr, lowres_input)\n",
    "    lowres_img = lowres_input.resize((w, h))\n",
    "    lowres_img_arr = img_to_array(lowres_img)\n",
    "    highres_img_arr = img_to_array(highres_img)\n",
    "    predict_img_arr_b = img_to_array(prediction_basic)\n",
    "    predict_img_arr_e = img_to_array(prediction_edsr)\n",
    "\n",
    "    baseline_rescaled = array_to_img(rescale(img_to_array(lowres_input), (upscale_factor, upscale_factor, 1)))\n",
    "    \n",
    "    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n",
    "    basic_psnr = tf.image.psnr(predict_img_arr_b, highres_img_arr, max_val=255)\n",
    "    edsr_psnr = tf.image.psnr(predict_img_arr_e, highres_img_arr, max_val=255)\n",
    "    baseline_psnr = tf.image.psnr(img_to_array(baseline_rescaled), highres_img_arr, max_val=255)\n",
    "\n",
    "    total_bicubic_psnr.append(bicubic_psnr)\n",
    "    total_edsr_psnr.append(edsr_psnr)\n",
    "    total_baseline_psnr.append(baseline_psnr)\n",
    "    total_basic_psnr.append(basic_psnr)\n",
    "\n",
    "    print(f\"Image {index}, path: {test_img_path}:\")\n",
    "    print(\n",
    "        \"\\tPSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n",
    "    )\n",
    "    print(\"\\tPSNR of baseline and high resolution is %.4f\" % baseline_psnr)\n",
    "    print(\"\\tPSNR of basic model prediction and high resolution is %.4f\" % basic_psnr)\n",
    "    print(\"\\tPSNR of EDSR model prediction and high resolution is %.4f\" % edsr_psnr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg. PSNR of lowres images is %.4f\" % np.mean(total_bicubic_psnr))\n",
    "print(\"Avg. PSNR of baseline is %.4f\" % np.mean(total_baseline_psnr))\n",
    "print(\"Avg. PSNR of reconstructions using basic model is %.4f\" % np.mean(total_basic_psnr))\n",
    "print(\"Avg. PSNR of reconstructions using EDSR model is %.4f\" % np.mean(total_edsr_psnr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = list(range(1, 101))\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "plt.title(\"Comparison of PSNR between models\")\n",
    "\n",
    "plt.plot(x_arr, total_bicubic_psnr, label=\"lowres\")\n",
    "plt.plot(x_arr, total_baseline_psnr, label=\"baseline\")\n",
    "plt.plot(x_arr, total_basic_psnr, label=\"basic\")\n",
    "plt.plot(x_arr, total_edsr_psnr, label=\"edsr\")\n",
    "\n",
    "plt.xticks(np.arange(1,101))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762d69e",
   "metadata": {},
   "source": [
    "As we can see from the metrics the baseline model produced the lowest quality images out of all methods presented. Its PSNR is even lower than the PSNR of the compressed image compared to the original. This is probably because the upscaling introduces more noise into the image. \n",
    "\n",
    "Surprisingly the more basic model managed to achieve better PSNR value than the more complex method. If we compare however the results on images then we can that the EDSR model produces better images than the basic model, as the lines are way sharper and the image overall feels less blurry. And the reason for the basic model getting better score than the more complex one is that that the smearing is usually preferred in PSNR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c71cf",
   "metadata": {},
   "source": [
    "## Problems and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865eea2f",
   "metadata": {},
   "source": [
    "First to write a functional utility for downloading, got stuck on this for a long time for no apparent reason.\n",
    "\n",
    "Designing how to use dataset - the DIV2K comes with already predownscaled images, however I had difficulties getting them to work with my model - as all inputs needed to be the same size for batching, I couldn't just open the images \"willy nilly\" and I needed a proper preprocessing - I decided to thus skip the predefined datasets and downscale the images myself, which made the work more berable and easier on me (and also made it that I can train on any provided dataset). \n",
    "\n",
    "Another problem was with defining the more complex model, as I got stuck on their explanation and it just took time and carefull thinking on how to connect it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d3a3d",
   "metadata": {},
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049986b2",
   "metadata": {},
   "source": [
    "Maybe try additional models, play with GANs and Autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0260e84",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
